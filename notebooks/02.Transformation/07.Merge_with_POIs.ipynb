{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 7. Merge Final con Puntos de Interés (POIs)\n",
    "\n",
    "Este notebook toma los archivos generados por `06.Merge_additional_files_duckdb.ipynb` y los enriquece con datos de puntos de interés (escuelas, hospitales, metro, etc.) utilizando una estrategia de vecino más cercano por coordenadas.\n",
    "\n",
    "El proceso es el siguiente:\n",
    "1. Itera sobre una lista de fechas para procesar múltiples archivos base.\n",
    "2. Para cada archivo base, carga una lista de CSVs con puntos de interés (POIs).\n",
    "3. Construye un índice geoespacial (`BallTree`) para cada conjunto de POIs para búsquedas eficientes.\n",
    "4. Procesa el archivo base en fragmentos (chunks) para controlar el uso de memoria.\n",
    "5. Para cada registro del archivo base, busca el POI más cercano dentro de un radio de 5 km.\n",
    "6. Agrega las columnas del POI encontrado y la distancia en metros al registro base.\n",
    "7. Guarda el resultado final enriquecido en un nuevo archivo Parquet para cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIGURACIÓN GENERAL ==============\n",
    "\n",
    "# Fechas de los archivos a procesar\n",
    "DATES_TO_PROCESS = [\n",
    "    \"2025-08-31\", \n",
    "    \"2025-09-14\", \n",
    "    \"2025-09-20\", \n",
    "    \"2025-09-27\", \n",
    "    \"2025-10-05\", \n",
    "    \"2025-10-11\"\n",
    "]\n",
    "\n",
    "# Rutas base\n",
    "BASE_DIR = Path(\"../../data/processed/\")\n",
    "CSV_FOLDER = BASE_DIR / \"csv\"\n",
    "OUTPUT_DIR = BASE_DIR / \"final_datasets\"\n",
    "TMP_CHUNK_DIR = OUTPUT_DIR / \"tmp_chunks\"\n",
    "\n",
    "# Archivos CSV de puntos de interés\n",
    "CSVS_INTERES = [\n",
    "    \"escuelas_privadas/escuelas_privadas_con_coordenadas.csv\",\n",
    "    \"hospitales_y_centros_de_salud/hospitales_y_centros_de_salud_con_coordenadas.csv\",\n",
    "    \"mb_shp/Metrobus_estaciones_con_coordenadas.csv\",\n",
    "    \"stcmetro_shp/STC_Metro_estaciones_utm14n_con_coordenadas.csv\",\n",
    "    \"areas_verdes_filtrado.csv\",\n",
    "    \"escuelas_publicas.csv\"\n",
    "]\n",
    "\n",
    "# Parámetros del proceso\n",
    "CHUNK_SIZE = 50_000\n",
    "MAX_DISTANCE_KM = 5.0\n",
    "R_EARTH_M = 6_371_000.0\n",
    "MAX_DISTANCE_RAD = (MAX_DISTANCE_KM * 1000.0) / R_EARTH_M\n",
    "\n",
    "# Crear directorios de salida\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_CHUNK_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Preparando índices para los CSVs de puntos de interés...\n",
      "✅ Índice listo: escuelas_privadas_con_coordenadas (3,659 puntos)\n",
      "✅ Índice listo: hospitales_y_centros_de_salud_con_coordenadas (27 puntos)\n",
      "✅ Índice listo: metrobus_estaciones_con_coordenadas (324 puntos)\n",
      "✅ Índice listo: stc_metro_estaciones_utm14n_con_coordenadas (195 puntos)\n",
      "✅ Índice listo: areas_verdes_filtrado (2,750 puntos)\n",
      "✅ Índice listo: escuelas_publicas (2,242 puntos)\n"
     ]
    }
   ],
   "source": [
    "# ============== PREPARAR ÍNDICES DE CSVs (BallTree) ==============\n",
    "print(\"🛠️ Preparando índices para los CSVs de puntos de interés...\")\n",
    "\n",
    "csv_models = []\n",
    "for rel_path in CSVS_INTERES:\n",
    "    csv_path = (CSV_FOLDER / rel_path).resolve()\n",
    "    alias = csv_path.stem.lower().replace(\" \", \"_\")\n",
    "    if not csv_path.exists():\n",
    "        print(f\"⚠️ CSV no encontrado, se omite: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if not {\"latitud\", \"longitud\"}.issubset(df.columns):\n",
    "        print(f\"⚠️ {csv_path.name} no tiene 'latitud' y 'longitud'. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    df = df.loc[df[\"latitud\"].notna() & df[\"longitud\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        print(f\"⚠️ {csv_path.name} sin coordenadas válidas. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    rename_map = {col: f\"{col}_{alias}\" for col in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    coords_deg = df[[f\"latitud_{alias}\", f\"longitud_{alias}\"]].astype(float).values\n",
    "    coords_rad = np.radians(coords_deg)\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    csv_models.append({\n",
    "        \"alias\": alias,\n",
    "        \"df\": df,\n",
    "        \"tree\": tree\n",
    "    })\n",
    "    print(f\"✅ Índice listo: {alias} ({len(df):,} puntos)\")\n",
    "\n",
    "if not csv_models:\n",
    "    raise RuntimeError(\"❌ No hay CSVs válidos para procesar. Abortando.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-08-31\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-08-31.parquet (4,094 filas)\n",
      "\n",
      "🧩 Chunk 0 → 4,094 (4,094 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 4,047 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 3,947 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,574 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,530 matches\n",
      "   ➕ areas_verdes_filtrado: 4,047 matches\n",
      "   ➕ escuelas_publicas: 4,047 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-08-31.parquet (4,094 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-09-14\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-14.parquet (4,197 filas)\n",
      "\n",
      "🧩 Chunk 0 → 4,197 (4,197 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 4,145 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 4,025 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,640 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,600 matches\n",
      "   ➕ areas_verdes_filtrado: 4,145 matches\n",
      "   ➕ escuelas_publicas: 4,145 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-09-14.parquet (4,197 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-09-20\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-20.parquet (3,982 filas)\n",
      "\n",
      "🧩 Chunk 0 → 3,982 (3,982 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 3,935 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 3,810 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,400 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,356 matches\n",
      "   ➕ areas_verdes_filtrado: 3,935 matches\n",
      "   ➕ escuelas_publicas: 3,935 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-09-20.parquet (3,982 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-09-27\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-27.parquet (4,180 filas)\n",
      "\n",
      "🧩 Chunk 0 → 4,180 (4,180 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 4,130 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 4,022 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,615 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,572 matches\n",
      "   ➕ areas_verdes_filtrado: 4,130 matches\n",
      "   ➕ escuelas_publicas: 4,130 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-09-27.parquet (4,180 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-10-05\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-10-05.parquet (4,038 filas)\n",
      "\n",
      "🧩 Chunk 0 → 4,038 (4,038 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 4,001 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 3,888 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,513 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,470 matches\n",
      "   ➕ areas_verdes_filtrado: 3,999 matches\n",
      "   ➕ escuelas_publicas: 4,001 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-10-05.parquet (4,038 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "🚀 Iniciando procesamiento para la fecha: 2025-10-11\n",
      "============================================================\n",
      "📦 Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-10-11.parquet (4,172 filas)\n",
      "\n",
      "🧩 Chunk 0 → 4,172 (4,172 filas)\n",
      "   ➕ escuelas_privadas_con_coordenadas: 4,134 matches\n",
      "   ➕ hospitales_y_centros_de_salud_con_coordenadas: 4,011 matches\n",
      "   ➕ metrobus_estaciones_con_coordenadas: 3,575 matches\n",
      "   ➕ stc_metro_estaciones_utm14n_con_coordenadas: 3,531 matches\n",
      "   ➕ areas_verdes_filtrado: 4,134 matches\n",
      "   ➕ escuelas_publicas: 4,134 matches\n",
      "\n",
      "📦 Combinando chunks para el archivo final...\n",
      "🎯 Archivo final guardado: final_merged_inmuebles24_2025-10-11.parquet (4,172 filas)\n",
      "🧹 Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "🎉 Proceso completado para todas las fechas.\n"
     ]
    }
   ],
   "source": [
    "# ============== BUCLE PRINCIPAL DE PROCESAMIENTO POR FECHA ==============\n",
    "\n",
    "for run_date in DATES_TO_PROCESS:\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"🚀 Iniciando procesamiento para la fecha: {run_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- 1. Definir rutas de entrada y salida ---\n",
    "    base_parquet = BASE_DIR / f\"merged_inmuebles24_departamentos_duckdb_cp_{run_date}.parquet\"\n",
    "    output_final = OUTPUT_DIR / f\"final_merged_inmuebles24_{run_date}.parquet\"\n",
    "\n",
    "    if not base_parquet.exists():\n",
    "        print(f\"❌ Archivo base no encontrado para la fecha {run_date}, omitiendo: {base_parquet}\")\n",
    "        continue\n",
    "\n",
    "    # --- 2. Conectar a DuckDB y obtener metadatos ---\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        n_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{base_parquet.as_posix()}')\").fetchone()[0]\n",
    "        print(f\"📦 Archivo base: {base_parquet.name} ({n_rows:,} filas)\")\n",
    "        \n",
    "        sample = con.execute(f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT 1\").fetchdf()\n",
    "        if not {\"longitud\", \"latitud\"}.issubset(sample.columns):\n",
    "            raise ValueError(\"El archivo base debe tener 'longitud' y 'latitud'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al leer el archivo base {base_parquet.name}: {e}\")\n",
    "        con.close()\n",
    "        continue\n",
    "\n",
    "    # --- 3. Procesar en chunks ---\n",
    "    chunk_paths = []\n",
    "    for start in range(0, n_rows, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, n_rows)\n",
    "        print(f\"\\n🧩 Chunk {start:,} → {end:,} ({end-start:,} filas)\")\n",
    "\n",
    "        base_chunk = con.execute(\n",
    "            f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT {end-start} OFFSET {start}\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        base_coords_rad = np.radians(base_chunk[[\"latitud\", \"longitud\"]].astype(float).values)\n",
    "\n",
    "        for model in csv_models:\n",
    "            alias = model[\"alias\"]\n",
    "            tree = model[\"tree\"]\n",
    "            df_csv = model[\"df\"]\n",
    "\n",
    "            dist_rad, idx = tree.query(base_coords_rad, k=1)\n",
    "            dist_rad, idx = dist_rad.flatten(), idx.flatten()\n",
    "\n",
    "            within_radius = dist_rad <= MAX_DISTANCE_RAD\n",
    "            matched_data = df_csv.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "            # Agregar distancia en metros\n",
    "            dist_m = dist_rad * R_EARTH_M\n",
    "            base_chunk[f\"dist_m_{alias}\"] = np.where(within_radius, dist_m, np.nan)\n",
    "\n",
    "            # Agregar coordenadas del POI\n",
    "            lat_col_poi = f\"latitud_{alias}\"\n",
    "            lon_col_poi = f\"longitud_{alias}\"\n",
    "\n",
    "            lat_vals = matched_data[lat_col_poi].copy()\n",
    "            lat_vals[~within_radius] = np.nan\n",
    "            base_chunk[f\"poi_lat_{alias}\"] = lat_vals.values\n",
    "\n",
    "            lon_vals = matched_data[lon_col_poi].copy()\n",
    "            lon_vals[~within_radius] = np.nan\n",
    "            base_chunk[f\"poi_lon_{alias}\"] = lon_vals.values\n",
    "\n",
    "            print(f\"   ➕ {alias}: {within_radius.sum():,} matches\")\n",
    "\n",
    "        out_path = TMP_CHUNK_DIR / f\"merged_chunk_{run_date}_{start:08d}.parquet\"\n",
    "        base_chunk.to_parquet(out_path, index=False)\n",
    "        chunk_paths.append(out_path)\n",
    "        del base_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # --- 4. Unir chunks y guardar final ---\n",
    "    if not chunk_paths:\n",
    "        print(\"🤷 No se generaron chunks, no hay nada que unir.\")\n",
    "        continue\n",
    "        \n",
    "    print(\"\\n📦 Combinando chunks para el archivo final...\")\n",
    "    final_df = pd.concat((pd.read_parquet(p) for p in chunk_paths), ignore_index=True)\n",
    "    final_df.to_parquet(output_final, index=False)\n",
    "    print(f\"🎯 Archivo final guardado: {output_final.name} ({len(final_df):,} filas)\")\n",
    "\n",
    "    # --- 5. Limpiar chunks temporales ---\n",
    "    for p in chunk_paths:\n",
    "        p.unlink()\n",
    "    print(\"🧹 Chunks temporales eliminados.\")\n",
    "\n",
    "print(\"\\n\\n🎉 Proceso completado para todas las fechas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a878c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
