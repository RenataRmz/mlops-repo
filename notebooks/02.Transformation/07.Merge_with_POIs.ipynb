{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 7. Merge Final con Puntos de InterÃ©s (POIs)\n",
    "\n",
    "Este notebook toma los archivos generados por `06.Merge_additional_files_duckdb.ipynb` y los enriquece con datos de puntos de interÃ©s (escuelas, hospitales, metro, etc.) utilizando una estrategia de vecino mÃ¡s cercano por coordenadas.\n",
    "\n",
    "El proceso es el siguiente:\n",
    "1. Itera sobre una lista de fechas para procesar mÃºltiples archivos base.\n",
    "2. Para cada archivo base, carga una lista de CSVs con puntos de interÃ©s (POIs).\n",
    "3. Construye un Ã­ndice geoespacial (`BallTree`) para cada conjunto de POIs para bÃºsquedas eficientes.\n",
    "4. Procesa el archivo base en fragmentos (chunks) para controlar el uso de memoria.\n",
    "5. Para cada registro del archivo base, busca el POI mÃ¡s cercano dentro de un radio de 5 km.\n",
    "6. Agrega las columnas del POI encontrado y la distancia en metros al registro base.\n",
    "7. Guarda el resultado final enriquecido en un nuevo archivo Parquet para cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIGURACIÃ“N GENERAL ==============\n",
    "\n",
    "# Fechas de los archivos a procesar\n",
    "DATES_TO_PROCESS = [\n",
    "    \"2025-08-31\", \n",
    "    \"2025-09-14\", \n",
    "    \"2025-09-20\", \n",
    "    \"2025-09-27\", \n",
    "    \"2025-10-05\", \n",
    "    \"2025-10-11\"\n",
    "]\n",
    "\n",
    "# Rutas base\n",
    "BASE_DIR = Path(\"../../data/processed/\")\n",
    "CSV_FOLDER = BASE_DIR / \"csv\"\n",
    "OUTPUT_DIR = BASE_DIR / \"final_datasets\"\n",
    "TMP_CHUNK_DIR = OUTPUT_DIR / \"tmp_chunks\"\n",
    "\n",
    "# Archivos CSV de puntos de interÃ©s\n",
    "CSVS_INTERES = [\n",
    "    \"escuelas_privadas/escuelas_privadas_con_coordenadas.csv\",\n",
    "    \"hospitales_y_centros_de_salud/hospitales_y_centros_de_salud_con_coordenadas.csv\",\n",
    "    \"mb_shp/Metrobus_estaciones_con_coordenadas.csv\",\n",
    "    \"stcmetro_shp/STC_Metro_estaciones_utm14n_con_coordenadas.csv\",\n",
    "    \"areas_verdes_filtrado.csv\",\n",
    "    \"escuelas_publicas.csv\"\n",
    "]\n",
    "\n",
    "# ParÃ¡metros del proceso\n",
    "CHUNK_SIZE = 50_000\n",
    "MAX_DISTANCE_KM = 5.0\n",
    "R_EARTH_M = 6_371_000.0\n",
    "MAX_DISTANCE_RAD = (MAX_DISTANCE_KM * 1000.0) / R_EARTH_M\n",
    "\n",
    "# Crear directorios de salida\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_CHUNK_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ Preparando Ã­ndices para los CSVs de puntos de interÃ©s...\n",
      "âœ… Ãndice listo: escuelas_privadas_con_coordenadas (3,659 puntos)\n",
      "âœ… Ãndice listo: hospitales_y_centros_de_salud_con_coordenadas (27 puntos)\n",
      "âœ… Ãndice listo: metrobus_estaciones_con_coordenadas (324 puntos)\n",
      "âœ… Ãndice listo: stc_metro_estaciones_utm14n_con_coordenadas (195 puntos)\n",
      "âœ… Ãndice listo: areas_verdes_filtrado (2,750 puntos)\n",
      "âœ… Ãndice listo: escuelas_publicas (2,242 puntos)\n"
     ]
    }
   ],
   "source": [
    "# ============== PREPARAR ÃNDICES DE CSVs (BallTree) ==============\n",
    "print(\"ğŸ› ï¸ Preparando Ã­ndices para los CSVs de puntos de interÃ©s...\")\n",
    "\n",
    "csv_models = []\n",
    "for rel_path in CSVS_INTERES:\n",
    "    csv_path = (CSV_FOLDER / rel_path).resolve()\n",
    "    alias = csv_path.stem.lower().replace(\" \", \"_\")\n",
    "    if not csv_path.exists():\n",
    "        print(f\"âš ï¸ CSV no encontrado, se omite: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if not {\"latitud\", \"longitud\"}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ {csv_path.name} no tiene 'latitud' y 'longitud'. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    df = df.loc[df[\"latitud\"].notna() & df[\"longitud\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        print(f\"âš ï¸ {csv_path.name} sin coordenadas vÃ¡lidas. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    rename_map = {col: f\"{col}_{alias}\" for col in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    coords_deg = df[[f\"latitud_{alias}\", f\"longitud_{alias}\"]].astype(float).values\n",
    "    coords_rad = np.radians(coords_deg)\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    csv_models.append({\n",
    "        \"alias\": alias,\n",
    "        \"df\": df,\n",
    "        \"tree\": tree\n",
    "    })\n",
    "    print(f\"âœ… Ãndice listo: {alias} ({len(df):,} puntos)\")\n",
    "\n",
    "if not csv_models:\n",
    "    raise RuntimeError(\"âŒ No hay CSVs vÃ¡lidos para procesar. Abortando.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-08-31\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-08-31.parquet (4,094 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 4,094 (4,094 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 4,047 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 3,947 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,574 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,530 matches\n",
      "   â• areas_verdes_filtrado: 4,047 matches\n",
      "   â• escuelas_publicas: 4,047 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-08-31.parquet (4,094 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-09-14\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-14.parquet (4,197 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 4,197 (4,197 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 4,145 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 4,025 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,640 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,600 matches\n",
      "   â• areas_verdes_filtrado: 4,145 matches\n",
      "   â• escuelas_publicas: 4,145 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-09-14.parquet (4,197 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-09-20\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-20.parquet (3,982 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 3,982 (3,982 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 3,935 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 3,810 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,400 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,356 matches\n",
      "   â• areas_verdes_filtrado: 3,935 matches\n",
      "   â• escuelas_publicas: 3,935 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-09-20.parquet (3,982 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-09-27\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-09-27.parquet (4,180 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 4,180 (4,180 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 4,130 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 4,022 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,615 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,572 matches\n",
      "   â• areas_verdes_filtrado: 4,130 matches\n",
      "   â• escuelas_publicas: 4,130 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-09-27.parquet (4,180 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-10-05\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-10-05.parquet (4,038 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 4,038 (4,038 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 4,001 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 3,888 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,513 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,470 matches\n",
      "   â• areas_verdes_filtrado: 3,999 matches\n",
      "   â• escuelas_publicas: 4,001 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-10-05.parquet (4,038 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Iniciando procesamiento para la fecha: 2025-10-11\n",
      "============================================================\n",
      "ğŸ“¦ Archivo base: merged_inmuebles24_departamentos_duckdb_cp_2025-10-11.parquet (4,172 filas)\n",
      "\n",
      "ğŸ§© Chunk 0 â†’ 4,172 (4,172 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: 4,134 matches\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: 4,011 matches\n",
      "   â• metrobus_estaciones_con_coordenadas: 3,575 matches\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: 3,531 matches\n",
      "   â• areas_verdes_filtrado: 4,134 matches\n",
      "   â• escuelas_publicas: 4,134 matches\n",
      "\n",
      "ğŸ“¦ Combinando chunks para el archivo final...\n",
      "ğŸ¯ Archivo final guardado: final_merged_inmuebles24_2025-10-11.parquet (4,172 filas)\n",
      "ğŸ§¹ Chunks temporales eliminados.\n",
      "\n",
      "\n",
      "ğŸ‰ Proceso completado para todas las fechas.\n"
     ]
    }
   ],
   "source": [
    "# ============== BUCLE PRINCIPAL DE PROCESAMIENTO POR FECHA ==============\n",
    "\n",
    "for run_date in DATES_TO_PROCESS:\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"ğŸš€ Iniciando procesamiento para la fecha: {run_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- 1. Definir rutas de entrada y salida ---\n",
    "    base_parquet = BASE_DIR / f\"merged_inmuebles24_departamentos_duckdb_cp_{run_date}.parquet\"\n",
    "    output_final = OUTPUT_DIR / f\"final_merged_inmuebles24_{run_date}.parquet\"\n",
    "\n",
    "    if not base_parquet.exists():\n",
    "        print(f\"âŒ Archivo base no encontrado para la fecha {run_date}, omitiendo: {base_parquet}\")\n",
    "        continue\n",
    "\n",
    "    # --- 2. Conectar a DuckDB y obtener metadatos ---\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        n_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{base_parquet.as_posix()}')\").fetchone()[0]\n",
    "        print(f\"ğŸ“¦ Archivo base: {base_parquet.name} ({n_rows:,} filas)\")\n",
    "        \n",
    "        sample = con.execute(f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT 1\").fetchdf()\n",
    "        if not {\"longitud\", \"latitud\"}.issubset(sample.columns):\n",
    "            raise ValueError(\"El archivo base debe tener 'longitud' y 'latitud'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al leer el archivo base {base_parquet.name}: {e}\")\n",
    "        con.close()\n",
    "        continue\n",
    "\n",
    "    # --- 3. Procesar en chunks ---\n",
    "    chunk_paths = []\n",
    "    for start in range(0, n_rows, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, n_rows)\n",
    "        print(f\"\\nğŸ§© Chunk {start:,} â†’ {end:,} ({end-start:,} filas)\")\n",
    "\n",
    "        base_chunk = con.execute(\n",
    "            f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT {end-start} OFFSET {start}\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        base_coords_rad = np.radians(base_chunk[[\"latitud\", \"longitud\"]].astype(float).values)\n",
    "\n",
    "        for model in csv_models:\n",
    "            alias = model[\"alias\"]\n",
    "            tree = model[\"tree\"]\n",
    "            df_csv = model[\"df\"]\n",
    "\n",
    "            dist_rad, idx = tree.query(base_coords_rad, k=1)\n",
    "            dist_rad, idx = dist_rad.flatten(), idx.flatten()\n",
    "\n",
    "            within_radius = dist_rad <= MAX_DISTANCE_RAD\n",
    "            matched_data = df_csv.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "            # Agregar distancia en metros\n",
    "            dist_m = dist_rad * R_EARTH_M\n",
    "            base_chunk[f\"dist_m_{alias}\"] = np.where(within_radius, dist_m, np.nan)\n",
    "\n",
    "            # Agregar coordenadas del POI\n",
    "            lat_col_poi = f\"latitud_{alias}\"\n",
    "            lon_col_poi = f\"longitud_{alias}\"\n",
    "\n",
    "            lat_vals = matched_data[lat_col_poi].copy()\n",
    "            lat_vals[~within_radius] = np.nan\n",
    "            base_chunk[f\"poi_lat_{alias}\"] = lat_vals.values\n",
    "\n",
    "            lon_vals = matched_data[lon_col_poi].copy()\n",
    "            lon_vals[~within_radius] = np.nan\n",
    "            base_chunk[f\"poi_lon_{alias}\"] = lon_vals.values\n",
    "\n",
    "            print(f\"   â• {alias}: {within_radius.sum():,} matches\")\n",
    "\n",
    "        out_path = TMP_CHUNK_DIR / f\"merged_chunk_{run_date}_{start:08d}.parquet\"\n",
    "        base_chunk.to_parquet(out_path, index=False)\n",
    "        chunk_paths.append(out_path)\n",
    "        del base_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # --- 4. Unir chunks y guardar final ---\n",
    "    if not chunk_paths:\n",
    "        print(\"ğŸ¤· No se generaron chunks, no hay nada que unir.\")\n",
    "        continue\n",
    "        \n",
    "    print(\"\\nğŸ“¦ Combinando chunks para el archivo final...\")\n",
    "    final_df = pd.concat((pd.read_parquet(p) for p in chunk_paths), ignore_index=True)\n",
    "    final_df.to_parquet(output_final, index=False)\n",
    "    print(f\"ğŸ¯ Archivo final guardado: {output_final.name} ({len(final_df):,} filas)\")\n",
    "\n",
    "    # --- 5. Limpiar chunks temporales ---\n",
    "    for p in chunk_paths:\n",
    "        p.unlink()\n",
    "    print(\"ğŸ§¹ Chunks temporales eliminados.\")\n",
    "\n",
    "print(\"\\n\\nğŸ‰ Proceso completado para todas las fechas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a878c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
