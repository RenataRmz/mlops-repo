{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "#!pip install duckdb\n",
    "#!pip install pandas\n",
    "#!pip install geopandas\n",
    "#!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0621d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c3883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se encontraron 33 archivos Parquet.\n",
      "\n",
      "â³ Combinando archivos...\n",
      "\n",
      "ğŸ’¾ Parquet combinado guardado en:\n",
      "/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb.parquet\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURACIÃ“N ===\n",
    "CARPETA_DATOS = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral\"\n",
    "ARCHIVO_SALIDA = os.path.join(CARPETA_DATOS, \"Delegacion_comb.parquet\")\n",
    "\n",
    "# === BUSCAR ARCHIVOS PARQUET (recursivamente) ===\n",
    "archivos_parquet = [\n",
    "    f for f in glob.glob(os.path.join(CARPETA_DATOS, \"**\", \"*.parquet\"), recursive=True)\n",
    "    if not f.endswith(\".crc\")\n",
    "]\n",
    "\n",
    "if not archivos_parquet:\n",
    "    print(\"âŒ No se encontraron archivos .parquet.\")\n",
    "else:\n",
    "    print(f\"âœ… Se encontraron {len(archivos_parquet)} archivos Parquet.\")\n",
    "    \n",
    "    # === CONECTAR A DUCKDB (sin archivo .db, todo en memoria) ===\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # === LEER Y COMBINAR ===\n",
    "    # DuckDB entiende patrones tipo *.parquet y concatena automÃ¡ticamente.\n",
    "    # Usamos UNION ALL para concatenar todos los archivos.\n",
    "    query = f\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM read_parquet({archivos_parquet})\n",
    "        )\n",
    "        TO '{ARCHIVO_SALIDA}'\n",
    "        (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nâ³ Combinando archivos...\")\n",
    "    con.execute(query)\n",
    "    con.close()\n",
    "\n",
    "    print(f\"\\nğŸ’¾ Parquet combinado guardado en:\\n{ARCHIVO_SALIDA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16e4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c862654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total original:  1,469,677\n",
      "âœ… Total combinado: 1,469,677\n",
      "ğŸ¯ Coinciden\n",
      "\n",
      "ğŸ§± Esquema:\n",
      "              column_name column_type null   key default extra\n",
      "0                     fid      DOUBLE  YES  None    None  None\n",
      "1                   fid_2      DOUBLE  YES  None    None  None\n",
      "2            calle_numero     VARCHAR  YES  None    None  None\n",
      "3           codigo_postal     VARCHAR  YES  None    None  None\n",
      "4                 colonia     VARCHAR  YES  None    None  None\n",
      "5                alcaldia     VARCHAR  YES  None    None  None\n",
      "6             sup_terreno      DOUBLE  YES  None    None  None\n",
      "7        sup_construccion      DOUBLE  YES  None    None  None\n",
      "8       anio_construccion      DOUBLE  YES  None    None  None\n",
      "9              instal_esp      BIGINT  YES  None    None  None\n",
      "10   valor_unitario_suelo      DOUBLE  YES  None    None  None\n",
      "11            valor_suelo      DOUBLE  YES  None    None  None\n",
      "12                cve_vus     VARCHAR  YES  None    None  None\n",
      "13               subsidio      DOUBLE  YES  None    None  None\n",
      "14             calle_norm     VARCHAR  YES  None    None  None\n",
      "15          nombre_limpio     VARCHAR  YES  None    None  None\n",
      "16       clave_delegacion     INTEGER  YES  None    None  None\n",
      "17        nombre_limpio_l     VARCHAR  YES  None    None  None\n",
      "18           ageb_ENTIDAD      BIGINT  YES  None    None  None\n",
      "19           ageb_NOM_ENT     VARCHAR  YES  None    None  None\n",
      "20           ageb_NOM_MUN     VARCHAR  YES  None    None  None\n",
      "21               ageb_LOC      BIGINT  YES  None    None  None\n",
      "22           ageb_NOM_LOC     VARCHAR  YES  None    None  None\n",
      "23              ageb_AGEB     VARCHAR  YES  None    None  None\n",
      "24               ageb_MZA      BIGINT  YES  None    None  None\n",
      "25            ageb_POBTOT      BIGINT  YES  None    None  None\n",
      "26            ageb_POBFEM     VARCHAR  YES  None    None  None\n",
      "27            ageb_POBMAS     VARCHAR  YES  None    None  None\n",
      "28             ageb_P_0A2     VARCHAR  YES  None    None  None\n",
      "29           ageb_P_0A2_F     VARCHAR  YES  None    None  None\n",
      "30           ageb_P_0A2_M     VARCHAR  YES  None    None  None\n",
      "31          ageb_P_18YMAS     VARCHAR  YES  None    None  None\n",
      "32        ageb_P_18YMAS_F     VARCHAR  YES  None    None  None\n",
      "33        ageb_P_18YMAS_M     VARCHAR  YES  None    None  None\n",
      "34             ageb_P_3A5     VARCHAR  YES  None    None  None\n",
      "35           ageb_P_3A5_F     VARCHAR  YES  None    None  None\n",
      "36           ageb_P_3A5_M     VARCHAR  YES  None    None  None\n",
      "37            ageb_P_6A11     VARCHAR  YES  None    None  None\n",
      "38          ageb_P_6A11_F     VARCHAR  YES  None    None  None\n",
      "39          ageb_P_6A11_M     VARCHAR  YES  None    None  None\n",
      "40            ageb_P_8A14     VARCHAR  YES  None    None  None\n",
      "41          ageb_P_8A14_F     VARCHAR  YES  None    None  None\n",
      "42          ageb_P_8A14_M     VARCHAR  YES  None    None  None\n",
      "43           ageb_P_12A14     VARCHAR  YES  None    None  None\n",
      "44         ageb_P_12A14_F     VARCHAR  YES  None    None  None\n",
      "45         ageb_P_12A14_M     VARCHAR  YES  None    None  None\n",
      "46           ageb_P_15A17     VARCHAR  YES  None    None  None\n",
      "47         ageb_P_15A17_F     VARCHAR  YES  None    None  None\n",
      "48         ageb_P_15A17_M     VARCHAR  YES  None    None  None\n",
      "49           ageb_P_18A24     VARCHAR  YES  None    None  None\n",
      "50         ageb_P_18A24_F     VARCHAR  YES  None    None  None\n",
      "51         ageb_P_18A24_M     VARCHAR  YES  None    None  None\n",
      "52         ageb_P_15A49_F     VARCHAR  YES  None    None  None\n",
      "53          ageb_P_60YMAS     VARCHAR  YES  None    None  None\n",
      "54        ageb_P_60YMAS_F     VARCHAR  YES  None    None  None\n",
      "55        ageb_P_60YMAS_M     VARCHAR  YES  None    None  None\n",
      "56         ageb_POB65_MAS     VARCHAR  YES  None    None  None\n",
      "57          ageb_P18YM_PB     VARCHAR  YES  None    None  None\n",
      "58        ageb_P18YM_PB_F     VARCHAR  YES  None    None  None\n",
      "59        ageb_P18YM_PB_M     VARCHAR  YES  None    None  None\n",
      "60            ageb_TOTHOG     VARCHAR  YES  None    None  None\n",
      "61          ageb_HOGJEF_F     VARCHAR  YES  None    None  None\n",
      "62          ageb_HOGJEF_M     VARCHAR  YES  None    None  None\n",
      "63            ageb_POBHOG     VARCHAR  YES  None    None  None\n",
      "64         ageb_PHOGJEF_F     VARCHAR  YES  None    None  None\n",
      "65         ageb_PHOGJEF_M     VARCHAR  YES  None    None  None\n",
      "66            ageb_VIVTOT      BIGINT  YES  None    None  None\n",
      "67           ageb_TVIVHAB     VARCHAR  YES  None    None  None\n",
      "68           ageb_TVIVPAR     VARCHAR  YES  None    None  None\n",
      "69        ageb_VIVPAR_HAB     VARCHAR  YES  None    None  None\n",
      "70        ageb_VIVPARH_CV     VARCHAR  YES  None    None  None\n",
      "71        ageb_TVIVPARHAB     VARCHAR  YES  None    None  None\n",
      "72        ageb_VIVPAR_DES     VARCHAR  YES  None    None  None\n",
      "73         ageb_VIVPAR_UT     VARCHAR  YES  None    None  None\n",
      "74        ageb_OCUPVIVPAR     VARCHAR  YES  None    None  None\n",
      "75         ageb_PROM_OCUP     VARCHAR  YES  None    None  None\n",
      "76        ageb_PRO_OCUP_C     VARCHAR  YES  None    None  None\n",
      "77          ageb_VPH_1DOR     VARCHAR  YES  None    None  None\n",
      "78        ageb_VPH_2YMASD     VARCHAR  YES  None    None  None\n",
      "79        ageb_VPH_1CUART     VARCHAR  YES  None    None  None\n",
      "80        ageb_VPH_2CUART     VARCHAR  YES  None    None  None\n",
      "81        ageb_VPH_3YMASC     VARCHAR  YES  None    None  None\n",
      "82        ageb_VPH_C_ELEC     VARCHAR  YES  None    None  None\n",
      "83        ageb_VPH_S_ELEC     VARCHAR  YES  None    None  None\n",
      "84        ageb_VPH_AGUADV     VARCHAR  YES  None    None  None\n",
      "85         ageb_VPH_AEASP     VARCHAR  YES  None    None  None\n",
      "86        ageb_VPH_AGUAFV     VARCHAR  YES  None    None  None\n",
      "87        ageb_VPH_TINACO     VARCHAR  YES  None    None  None\n",
      "88        ageb_VPH_CISTER     VARCHAR  YES  None    None  None\n",
      "89         ageb_VPH_EXCSA     VARCHAR  YES  None    None  None\n",
      "90          ageb_VPH_LETR     VARCHAR  YES  None    None  None\n",
      "91        ageb_VPH_DRENAJ     VARCHAR  YES  None    None  None\n",
      "92        ageb_VPH_NODREN     VARCHAR  YES  None    None  None\n",
      "93        ageb_VPH_C_SERV     VARCHAR  YES  None    None  None\n",
      "94        ageb_VPH_NDEAED     VARCHAR  YES  None    None  None\n",
      "95        ageb_VPH_DSADMA     VARCHAR  YES  None    None  None\n",
      "96        ageb_VPH_NDACMM     VARCHAR  YES  None    None  None\n",
      "97        ageb_VPH_SNBIEN     VARCHAR  YES  None    None  None\n",
      "98         ageb_VPH_REFRI     VARCHAR  YES  None    None  None\n",
      "99         ageb_VPH_LAVAD     VARCHAR  YES  None    None  None\n",
      "100       ageb_VPH_HMICRO     VARCHAR  YES  None    None  None\n",
      "101        ageb_VPH_AUTOM     VARCHAR  YES  None    None  None\n",
      "102         ageb_VPH_MOTO     VARCHAR  YES  None    None  None\n",
      "103         ageb_VPH_BICI     VARCHAR  YES  None    None  None\n",
      "104        ageb_VPH_RADIO     VARCHAR  YES  None    None  None\n",
      "105           ageb_VPH_TV     VARCHAR  YES  None    None  None\n",
      "106           ageb_VPH_PC     VARCHAR  YES  None    None  None\n",
      "107        ageb_VPH_TELEF     VARCHAR  YES  None    None  None\n",
      "108          ageb_VPH_CEL     VARCHAR  YES  None    None  None\n",
      "109        ageb_VPH_INTER     VARCHAR  YES  None    None  None\n",
      "110         ageb_VPH_STVP     VARCHAR  YES  None    None  None\n",
      "111       ageb_VPH_SPMVPI     VARCHAR  YES  None    None  None\n",
      "112          ageb_VPH_CVJ     VARCHAR  YES  None    None  None\n",
      "113       ageb_VPH_SINRTV     VARCHAR  YES  None    None  None\n",
      "114       ageb_VPH_SINLTC     VARCHAR  YES  None    None  None\n",
      "115      ageb_VPH_SINCINT     VARCHAR  YES  None    None  None\n",
      "116       ageb_VPH_SINTIC     VARCHAR  YES  None    None  None\n",
      "117              ageb_key     VARCHAR  YES  None    None  None\n",
      "118       ageb_ENTIDAD_fm      DOUBLE  YES  None    None  None\n",
      "119           ageb_MUN_fm      DOUBLE  YES  None    None  None\n",
      "120           ageb_LOC_fm      DOUBLE  YES  None    None  None\n",
      "121          ageb_AGEB_fm     VARCHAR  YES  None    None  None\n",
      "122           ageb_MZA_fm      DOUBLE  YES  None    None  None\n",
      "123          ageb_CVEVIAL      DOUBLE  YES  None    None  None\n",
      "124           ageb_CVESEG      DOUBLE  YES  None    None  None\n",
      "125            ageb_CVEFT      DOUBLE  YES  None    None  None\n",
      "126          ageb_NOMVIAL     VARCHAR  YES  None    None  None\n",
      "127         ageb_TIPOVIAL     VARCHAR  YES  None    None  None\n",
      "128         ageb_CVEVIAL1     VARCHAR  YES  None    None  None\n",
      "129          ageb_CVESEG1     VARCHAR  YES  None    None  None\n",
      "130          ageb_CVEREF1      DOUBLE  YES  None    None  None\n",
      "131          ageb_TIPOVR1     VARCHAR  YES  None    None  None\n",
      "132          ageb_NOMREF1     VARCHAR  YES  None    None  None\n",
      "133         ageb_CVEVIAL2     VARCHAR  YES  None    None  None\n",
      "134          ageb_CVESEG2     VARCHAR  YES  None    None  None\n",
      "135          ageb_CVEREF2      DOUBLE  YES  None    None  None\n",
      "136          ageb_TIPOVR2     VARCHAR  YES  None    None  None\n",
      "137          ageb_NOMREF2     VARCHAR  YES  None    None  None\n",
      "138         ageb_CVEVIAL3      DOUBLE  YES  None    None  None\n",
      "139          ageb_CVESEG3      DOUBLE  YES  None    None  None\n",
      "140          ageb_CVEREF3      DOUBLE  YES  None    None  None\n",
      "141          ageb_TIPOVR3     VARCHAR  YES  None    None  None\n",
      "142          ageb_NOMREF3     VARCHAR  YES  None    None  None\n",
      "143              ageb_lon      DOUBLE  YES  None    None  None\n",
      "144              ageb_lat      DOUBLE  YES  None    None  None\n",
      "145              ageb_MUN     INTEGER  YES  None    None  None\n",
      "146        ageb_nomref1_l     VARCHAR  YES  None    None  None\n",
      "147            ageb_block     VARCHAR  YES  None    None  None\n",
      "148              ageb_len     INTEGER  YES  None    None  None\n",
      "149       matched_NOMREF1     VARCHAR  YES  None    None  None\n",
      "150           similaridad      DOUBLE  YES  None    None  None\n"
     ]
    }
   ],
   "source": [
    "archivos = [\n",
    "    f for f in glob.glob(os.path.join(CARPETA_DATOS, \"**\", \"*.parquet\"), recursive=True)\n",
    "    if not f.endswith(\".crc\") and not f.endswith(\"Delegacion_comb.parquet\")\n",
    "]\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Total de filas en todos los archivos originales\n",
    "total_originales = con.execute(f\"SELECT SUM(count) FROM (SELECT COUNT(*) AS count FROM read_parquet({archivos}) GROUP BY filename)\").fetchone()[0]\n",
    "\n",
    "# Total en el combinado\n",
    "total_combinado = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{ARCHIVO_SALIDA}')\").fetchone()[0]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Total original:  {total_originales:,}\")\n",
    "print(f\"âœ… Total combinado: {total_combinado:,}\")\n",
    "print(\"ğŸ¯ Coinciden\" if total_originales == total_combinado else \"âš ï¸ No coinciden, revisa los archivos.\")\n",
    "\n",
    "# Ver las columnas y tipos\n",
    "print(\"\\nğŸ§± Esquema:\")\n",
    "print(con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{ARCHIVO_SALIDA}')\").fetchdf())\n",
    "\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b5ab3",
   "metadata": {},
   "source": [
    "UtilizarÃ© \"ageb_lon\" y \"ageb_lat\" para hacer el merge con las otras basesde datos.\n",
    "Antes quitarÃ© los nulos de mi base final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caff00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total  sin_coordenadas  con_coordenadas\n",
      "0  1469677             25.0        1469652.0\n"
     ]
    }
   ],
   "source": [
    "ARCHIVO = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb.parquet\"\n",
    "con = duckdb.connect()\n",
    "\n",
    "stats = con.execute(f\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total,\n",
    "  SUM(CASE WHEN ageb_lon IS NULL OR ageb_lat IS NULL THEN 1 ELSE 0 END) AS sin_coordenadas,\n",
    "  SUM(CASE WHEN ageb_lon IS NOT NULL AND ageb_lat IS NOT NULL THEN 1 ELSE 0 END) AS con_coordenadas\n",
    "FROM read_parquet('{ARCHIVO}')\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(stats)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d89f6f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Filtrando datos...\n",
      "âœ… Filtrado completo. Total de filas con coordenadas: 1,469,652\n"
     ]
    }
   ],
   "source": [
    "CARPETA_DATOS = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral\"\n",
    "ARCHIVO_ORIGINAL = os.path.join(CARPETA_DATOS, \"Delegacion_comb.parquet\")\n",
    "ARCHIVO_FILTRADO = os.path.join(CARPETA_DATOS, \"Delegacion_comb_filtrado.parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Crear una nueva versiÃ³n filtrada (sin valores nulos en lat/lon)\n",
    "query = f\"\"\"\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM read_parquet('{ARCHIVO_ORIGINAL}')\n",
    "    WHERE ageb_lon IS NOT NULL AND ageb_lat IS NOT NULL\n",
    ")\n",
    "TO '{ARCHIVO_FILTRADO}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "\"\"\"\n",
    "\n",
    "print(\"â³ Filtrando datos...\")\n",
    "con.execute(query)\n",
    "\n",
    "# Verificar resultado\n",
    "result = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{ARCHIVO_FILTRADO}')\").fetchone()[0]\n",
    "print(f\"âœ… Filtrado completo. Total de filas con coordenadas: {result:,}\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076899c",
   "metadata": {},
   "source": [
    "Ahora cambiare el formato de las latitudes y longitudes del Concat de las delegaciones. de INEGI (EPSG:6372) â†’ WGS84 (EPSG:4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dcbd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55b9fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageb_path = \"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c6b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERSIÃ“N DE COORDENADAS INEGI (CCL ITRF2008) â†’ WGS84\n",
    "# ============================================\n",
    "\n",
    "# === 1ï¸âƒ£ CONFIGURACIÃ“N ===\n",
    "parquet_path = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado.parquet\"\n",
    "output_path = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado_wgs84.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a21147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Cargando archivo Parquet: /Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado.parquet\n",
      "âœ… Archivo cargado: 1,469,652 filas, 151 columnas\n"
     ]
    }
   ],
   "source": [
    "# === 2ï¸âƒ£ LEER PARQUET ===\n",
    "print(f\"ğŸ“¦ Cargando archivo Parquet: {parquet_path}\")\n",
    "df_ageb = pd.read_parquet(parquet_path)\n",
    "print(f\"âœ… Archivo cargado: {df_ageb.shape[0]:,} filas, {df_ageb.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650ab33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3ï¸âƒ£ CREAR TRANSFORMADOR ===\n",
    "# INEGI usa Lambert CÃ³nica Conforme ITRF2008 = EPSG:6372\n",
    "transformer = Transformer.from_crs(\"EPSG:6372\", \"EPSG:4326\", always_xy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "034335c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Filas vÃ¡lidas con coordenadas: 1,469,652\n"
     ]
    }
   ],
   "source": [
    "# === 4ï¸âƒ£ VALIDAR COLUMNAS ===\n",
    "# Ajusta si tus columnas tienen otros nombres\n",
    "if not {\"ageb_lon\", \"ageb_lat\"}.issubset(df_ageb.columns):\n",
    "    raise ValueError(\"âŒ El archivo debe tener columnas llamadas 'ageb_lon' y 'ageb_lat'.\")\n",
    "\n",
    "# === 5ï¸âƒ£ FILTRAR FILAS VÃLIDAS ===\n",
    "mask_valid = df_ageb[\"ageb_lon\"].notna() & df_ageb[\"ageb_lat\"].notna()\n",
    "print(f\"ğŸ“ Filas vÃ¡lidas con coordenadas: {mask_valid.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2efec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6ï¸âƒ£ TRANSFORMAR COORDENADAS ===\n",
    "lon_wgs84, lat_wgs84 = transformer.transform(\n",
    "    df_ageb.loc[mask_valid, \"ageb_lon\"].astype(float).values,\n",
    "    df_ageb.loc[mask_valid, \"ageb_lat\"].astype(float).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5c7b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Ejemplo de coordenadas transformadas:\n",
      "       ageb_lon       ageb_lat  lon_wgs84  lat_wgs84\n",
      "0  2.800048e+06  845015.333800 -99.131257  19.577183\n",
      "1  2.800048e+06  845015.333800 -99.131257  19.577183\n",
      "2  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "3  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "4  2.800955e+06  843250.986300 -99.122927  19.561034\n",
      "5  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "6  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "7  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "8  2.800006e+06  844827.171348 -99.131691  19.575486\n",
      "9  2.800006e+06  844827.171348 -99.131691  19.575486\n"
     ]
    }
   ],
   "source": [
    "# === 7ï¸âƒ£ CREAR NUEVAS COLUMNAS ===\n",
    "df_ageb[\"lon_wgs84\"] = pd.NA\n",
    "df_ageb[\"lat_wgs84\"] = pd.NA\n",
    "df_ageb.loc[mask_valid, \"lon_wgs84\"] = lon_wgs84\n",
    "df_ageb.loc[mask_valid, \"lat_wgs84\"] = lat_wgs84\n",
    "\n",
    "# === 8ï¸âƒ£ VISTA PREVIA ===\n",
    "print(\"\\nğŸ” Ejemplo de coordenadas transformadas:\")\n",
    "print(df_ageb[[\"ageb_lon\", \"ageb_lat\", \"lon_wgs84\", \"lat_wgs84\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "949e232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Archivo guardado con coordenadas WGS84 en: /Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado_wgs84.parquet\n",
      "   Columnas agregadas: 'lon_wgs84', 'lat_wgs84'\n"
     ]
    }
   ],
   "source": [
    "# === 9ï¸âƒ£ GUARDAR PARQUET ACTUALIZADO ===\n",
    "# Usa compresiÃ³n Snappy (por defecto) para eficiencia\n",
    "df_ageb.to_parquet(output_path, index=False)\n",
    "print(f\"\\nâœ… Archivo guardado con coordenadas WGS84 en: {output_path}\")\n",
    "print(\"   Columnas agregadas: 'lon_wgs84', 'lat_wgs84'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95301257",
   "metadata": {},
   "source": [
    "Voy a renombrar las columnas de los archivos csv de los puntos de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "864443e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÃ“N ===\n",
    "RUTA_BASE = r\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/csv\"\n",
    "SOBREESCRIBIR = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32a0b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posibles nombres de columnas para detecciÃ³n flexible\n",
    "NOMBRES_LATITUD = [\"lat\", \"latitude\", \"latitud\", \"y\"]\n",
    "NOMBRES_LONGITUD = [\"lon\", \"long\", \"longitude\", \"longitud\", \"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b551a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCIÃ“N PRINCIPAL ===\n",
    "def renombrar_columnas_csv(ruta_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ No se pudo leer {ruta_csv}: {e}\")\n",
    "        return\n",
    "\n",
    "    columnas_originales = list(df.columns)\n",
    "    columnas_lower = [c.strip().lower() for c in columnas_originales]\n",
    "\n",
    "    renombrar = {}\n",
    "\n",
    "    # Buscar equivalencias en nombres de columnas\n",
    "    for i, nombre in enumerate(columnas_lower):\n",
    "        if nombre in [n.lower() for n in NOMBRES_LATITUD]:\n",
    "            renombrar[columnas_originales[i]] = \"latitud\"\n",
    "        elif nombre in [n.lower() for n in NOMBRES_LONGITUD]:\n",
    "            renombrar[columnas_originales[i]] = \"longitud\"\n",
    "\n",
    "    if renombrar:\n",
    "        print(f\"\\nğŸ“„ Procesando: {os.path.basename(ruta_csv)}\")\n",
    "        print(f\"ğŸ” Renombrando columnas: {renombrar}\")\n",
    "        df.rename(columns=renombrar, inplace=True)\n",
    "\n",
    "        # Guardar sobrescribiendo\n",
    "        df.to_csv(ruta_csv, index=False)\n",
    "        print(f\"âœ… Archivo sobrescrito: {ruta_csv}\")\n",
    "\n",
    "        # Validar coordenadas si existen ambas columnas\n",
    "        if \"latitud\" in df.columns and \"longitud\" in df.columns:\n",
    "            try:\n",
    "                lat_min, lat_max = df[\"latitud\"].min(), df[\"latitud\"].max()\n",
    "                lon_min, lon_max = df[\"longitud\"].min(), df[\"longitud\"].max()\n",
    "\n",
    "                print(f\"ğŸ“Š Rango latitud:  {lat_min:.6f} â†’ {lat_max:.6f}\")\n",
    "                print(f\"ğŸ“Š Rango longitud: {lon_min:.6f} â†’ {lon_max:.6f}\")\n",
    "\n",
    "                # DetecciÃ³n bÃ¡sica de formato\n",
    "                if abs(lat_min) > 90 or abs(lat_max) > 90 or abs(lon_min) > 180 or abs(lon_max) > 180:\n",
    "                    print(\"âš ï¸ Coordenadas fuera de rango tÃ­pico (posiblemente en metros o en proyecciÃ³n).\")\n",
    "                else:\n",
    "                    print(\"âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ No se pudo calcular rango de coordenadas: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No se encontraron ambas columnas de latitud y longitud despuÃ©s del renombrado.\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No se detectaron columnas para renombrar en {os.path.basename(ruta_csv)}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4915b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No se detectaron columnas para renombrar en cdmx_areas_verdes_2017.csv.\n",
      "\n",
      "ğŸ“„ Procesando: areas_verdes_filtrado.csv\n",
      "ğŸ” Renombrando columnas: {'latitud': 'latitud', 'longitud': 'longitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/areas_verdes_filtrado.csv\n",
      "ğŸ“Š Rango latitud:  19.173251 â†’ 19.562414\n",
      "ğŸ“Š Rango longitud: -99.331159 â†’ -98.962573\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: escuelas_publicas.csv\n",
      "ğŸ” Renombrando columnas: {'latitud': 'latitud', 'longitud': 'longitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/escuelas_publicas.csv\n",
      "ğŸ“Š Rango latitud:  19.169883 â†’ 19.577081\n",
      "ğŸ“Š Rango longitud: -99.334459 â†’ -98.951824\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: escuelas_privadas_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/escuelas_privadas/escuelas_privadas_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  19.178203 â†’ 19.576274\n",
      "ğŸ“Š Rango longitud: -99.335725 â†’ -98.961798\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: hospitales_y_centros_de_salud_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/hospitales_y_centros_de_salud/hospitales_y_centros_de_salud_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  19.199308 â†’ 19.539803\n",
      "ğŸ“Š Rango longitud: -99.301117 â†’ -98.997925\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: Metrobus_lineas_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/mb_shp/Metrobus_lineas_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  19.358605 â†’ 19.500096\n",
      "ğŸ“Š Rango longitud: -99.186767 â†’ -99.054110\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: Metrobus_estaciones_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/mb_shp/Metrobus_estaciones_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  19.273374 â†’ 19.528617\n",
      "ğŸ“Š Rango longitud: -99.199546 â†’ -99.047613\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: STC_Metro_estaciones_utm14n_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/stcmetro_shp/STC_Metro_estaciones_utm14n_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  19.286022 â†’ 19.534513\n",
      "ğŸ“Š Rango longitud: -99.215842 â†’ -98.960937\n",
      "âœ… Coordenadas parecen estar en grados geogrÃ¡ficos (WGS84).\n",
      "\n",
      "ğŸ“„ Procesando: STC_Metro_lineas_utm14n_con_coordenadas.csv\n",
      "ğŸ” Renombrando columnas: {'lon': 'longitud', 'lat': 'latitud'}\n",
      "âœ… Archivo sobrescrito: /Users/eaha/Documents/TFM/mlops-repo/data/processed/csv/stcmetro_shp/STC_Metro_lineas_utm14n_con_coordenadas.csv\n",
      "ğŸ“Š Rango latitud:  2137763.309155 â†’ 2155008.497780\n",
      "ğŸ“Š Rango longitud: 479986.843540 â†’ 497757.925961\n",
      "âš ï¸ Coordenadas fuera de rango tÃ­pico (posiblemente en metros o en proyecciÃ³n).\n"
     ]
    }
   ],
   "source": [
    "# === RECORRER SUBCARPETAS Y APLICAR ===\n",
    "for root, _, files in os.walk(RUTA_BASE):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".csv\"):\n",
    "            ruta_completa = os.path.join(root, file)\n",
    "            renombrar_columnas_csv(ruta_completa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c283c35",
   "metadata": {},
   "source": [
    "Ahora ya tengo listos todos mis data sets, procederÃ© a realizar el merge de mi archivo \n",
    "Delegacion_comb_filtrado_wgs84.parquet como base y agregarle los datos de todos los otros csv que encontramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e428cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb74a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIGURACIÃ“N ==============\n",
    "base_parquet = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado_wgs84.parquet\")\n",
    "carpeta_csvs = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/csv\")\n",
    "csvs_interes = [\n",
    "    \"escuelas_privadas/escuelas_privadas_con_coordenadas.csv\",\n",
    "    \"hospitales_y_centros_de_salud/hospitales_y_centros_de_salud_con_coordenadas.csv\",\n",
    "    \"mb_shp/Metrobus_estaciones_con_coordenadas.csv\",\n",
    "    \"stcmetro_shp/STC_Metro_estaciones_utm14n_con_coordenadas.csv\",\n",
    "    \"areas_verdes_filtrado.csv\",\n",
    "    \"escuelas_publicas.csv\"\n",
    "]\n",
    "output_dir = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/tmp_chunks_balltree\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_final = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Merged_Delegacion_final_balltree.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e3a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50_000           # ajusta si te hace falta menos RAM (p.ej. 20_000)\n",
    "max_distance_km = 5.0         # radio de match\n",
    "R_EARTH_M = 6_371_000.0       # radio tierra (m)\n",
    "max_distance_m = max_distance_km * 1000.0\n",
    "max_distance_rad = max_distance_m / R_EARTH_M  # para mÃ©trica haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8810bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Archivo base: 1,469,652 filas\n"
     ]
    }
   ],
   "source": [
    "# ============== CARGAR BASE ==============\n",
    "# Usamos DuckDB solo para leer la base en chunks (pandas no streamea parquet por filas fÃ¡cilmente)\n",
    "con = duckdb.connect()\n",
    "n_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{base_parquet.as_posix()}')\").fetchone()[0]\n",
    "print(f\"ğŸ“¦ Archivo base: {n_rows:,} filas\")\n",
    "\n",
    "# Validaciones mÃ­nimas\n",
    "sample = con.execute(f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT 1\").fetchdf()\n",
    "if not {\"lon_wgs84\", \"lat_wgs84\"}.issubset(sample.columns):\n",
    "    raise ValueError(\"âŒ El archivo base debe tener columnas 'lon_wgs84' y 'lat_wgs84'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d600751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ Ãndice listo: escuelas_privadas_con_coordenadas (3,659 puntos)\n",
      "ğŸ› ï¸ Ãndice listo: hospitales_y_centros_de_salud_con_coordenadas (27 puntos)\n",
      "ğŸ› ï¸ Ãndice listo: metrobus_estaciones_con_coordenadas (324 puntos)\n",
      "ğŸ› ï¸ Ãndice listo: stc_metro_estaciones_utm14n_con_coordenadas (195 puntos)\n",
      "ğŸ› ï¸ Ãndice listo: areas_verdes_filtrado (2,750 puntos)\n",
      "ğŸ› ï¸ Ãndice listo: escuelas_publicas (2,242 puntos)\n"
     ]
    }
   ],
   "source": [
    "# ============== PREPARAR CSVs (Ã­ndice BallTree por archivo) ==============\n",
    "csv_models = []  # lista de dicts: {alias, df, tree, coords_rad, cols_out}\n",
    "for rel in csvs_interes:\n",
    "    csv_path = (carpeta_csvs / rel).resolve()\n",
    "    alias = csv_path.stem.lower().replace(\" \", \"_\")\n",
    "    if not csv_path.exists():\n",
    "        print(f\"âš ï¸ CSV no encontrado, se omite: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Validar columnas esperadas\n",
    "    if not {\"latitud\", \"longitud\"}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ {csv_path.name} no tiene columnas 'latitud' y 'longitud'. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    # Filtrar filas con coordenadas vÃ¡lidas\n",
    "    df = df.loc[df[\"latitud\"].notna() & df[\"longitud\"].notna()].copy()\n",
    "    if df.empty:\n",
    "        print(f\"âš ï¸ {csv_path.name} sin coordenadas vÃ¡lidas. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    # Sufijos: renombrar TODAS las columnas del CSV (incluyendo lat/long) para evitar colisiones\n",
    "    rename_map = {col: f\"{col}_{alias}\" for col in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Coordenadas en radianes (orden lat, lon para haversine)\n",
    "    coords_deg = df[[f\"latitud_{alias}\", f\"longitud_{alias}\"]].astype(float).values\n",
    "    coords_rad = np.radians(coords_deg)\n",
    "    # BallTree con mÃ©trica haversine\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    # Guardar modelo del CSV\n",
    "    csv_models.append({\n",
    "        \"alias\": alias,\n",
    "        \"df\": df,                    # dataframe con columnas ya sufijadas\n",
    "        \"tree\": tree,\n",
    "        \"coords_rad\": coords_rad,\n",
    "        \"cols_out\": df.columns.tolist()  # todas las columnas del CSV (sufijadas)\n",
    "    })\n",
    "    print(f\"ğŸ› ï¸ Ãndice listo: {alias} ({len(df):,} puntos)\")\n",
    "\n",
    "if not csv_models:\n",
    "    raise RuntimeError(\"No hay CSVs vÃ¡lidos para procesar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae071ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Chunk 0 â†’ 50,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 46,191/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 38,269/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00000000_00050000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 50,000 â†’ 100,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 46,373/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 41,068/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00050000_00100000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 100,000 â†’ 150,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 32,697/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 32,340/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 22,179/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00100000_00150000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 150,000 â†’ 200,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 48,387/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 49,498/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 49,482/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00150000_00200000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 200,000 â†’ 250,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 47,547/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 41,194/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 41,395/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00200000_00250000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 250,000 â†’ 300,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 48,665/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 48,750/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 49,443/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00250000_00300000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 300,000 â†’ 350,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 49,453/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 49,388/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 49,905/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00300000_00350000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 350,000 â†’ 400,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 45,873/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00350000_00400000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 400,000 â†’ 450,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 49,956/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 25,488/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 46,067/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 36,321/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 49,543/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 49,956/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00400000_00450000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 450,000 â†’ 500,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 49,749/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 6,329/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 42,929/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 19,815/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 48,554/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 49,749/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00450000_00500000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 500,000 â†’ 550,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 49,838/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 20,758/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 43,357/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 18,261/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 48,543/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 49,838/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00500000_00550000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 550,000 â†’ 600,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 49,568/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 40,421/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 23,400/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 49,236/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00550000_00600000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 600,000 â†’ 650,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00600000_00650000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 650,000 â†’ 700,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00650000_00700000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 700,000 â†’ 750,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00700000_00750000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 750,000 â†’ 800,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00750000_00800000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 800,000 â†’ 850,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00800000_00850000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 850,000 â†’ 900,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00850000_00900000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 900,000 â†’ 950,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00900000_00950000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 950,000 â†’ 1,000,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 48,561/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 20,499/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 20,499/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_00950000_01000000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,000,000 â†’ 1,050,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 44,151/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 41,074/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 47,131/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01000000_01050000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,050,000 â†’ 1,100,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 46,204/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 34,685/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01050000_01100000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,100,000 â†’ 1,150,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 44,190/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 38,049/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01100000_01150000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,150,000 â†’ 1,200,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 45,599/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 29,967/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 49,732/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01150000_01200000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,200,000 â†’ 1,250,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 48,578/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 440/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 46,397/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01200000_01250000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,250,000 â†’ 1,300,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01250000_01300000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,300,000 â†’ 1,350,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 49,884/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 41,875/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 45,193/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01300000_01350000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,350,000 â†’ 1,400,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 49,604/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 36,150/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 38,749/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01350000_01400000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,400,000 â†’ 1,450,000 (50,000 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 49,495/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 34,723/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 39,573/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 50,000/50,000 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01400000_01450000.parquet (50,000 filas)\n",
      "\n",
      "ğŸ§© Chunk 1,450,000 â†’ 1,469,652 (19,652 filas)\n",
      "   â• escuelas_privadas_con_coordenadas: match en 19,652/19,652 filas (radio â‰¤ 5.0 km)\n",
      "   â• hospitales_y_centros_de_salud_con_coordenadas: match en 19,102/19,652 filas (radio â‰¤ 5.0 km)\n",
      "   â• metrobus_estaciones_con_coordenadas: match en 11,716/19,652 filas (radio â‰¤ 5.0 km)\n",
      "   â• stc_metro_estaciones_utm14n_con_coordenadas: match en 12,989/19,652 filas (radio â‰¤ 5.0 km)\n",
      "   â• areas_verdes_filtrado: match en 19,652/19,652 filas (radio â‰¤ 5.0 km)\n",
      "   â• escuelas_publicas: match en 19,652/19,652 filas (radio â‰¤ 5.0 km)\n",
      "âœ… Guardado: merged_chunk_01450000_01469652.parquet (19,652 filas)\n"
     ]
    }
   ],
   "source": [
    "# ============== PROCESAR EN CHUNKS ==============\n",
    "chunk_paths = []\n",
    "for start in range(0, n_rows, chunk_size):\n",
    "    end = min(start + chunk_size, n_rows)\n",
    "    print(f\"\\nğŸ§© Chunk {start:,} â†’ {end:,} ({end-start:,} filas)\")\n",
    "\n",
    "    base_chunk = con.execute(\n",
    "        f\"SELECT * FROM read_parquet('{base_parquet.as_posix()}') LIMIT {end-start} OFFSET {start}\"\n",
    "    ).fetchdf()\n",
    "\n",
    "    # Nos aseguramos de no perder el Ã­ndice original si lo necesitas\n",
    "    # base_chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Coordenadas del base en radianes (orden lat, lon)\n",
    "    base_coords_deg = base_chunk[[\"lat_wgs84\", \"lon_wgs84\"]].astype(float).values\n",
    "    base_coords_rad = np.radians(base_coords_deg)\n",
    "\n",
    "    # Para cada CSV, buscamos el vecino mÃ¡s cercano y aÃ±adimos columnas\n",
    "    for model in csv_models:\n",
    "        alias = model[\"alias\"]\n",
    "        tree = model[\"tree\"]\n",
    "        df_csv = model[\"df\"]  # columnas sufijadas, incluido lat/long\n",
    "\n",
    "        # k=1 vecino mÃ¡s cercano, devuelve distancia (rad) y el Ã­ndice del CSV\n",
    "        dist_rad, idx = tree.query(base_coords_rad, k=1)\n",
    "        dist_rad = dist_rad.reshape(-1)\n",
    "        idx = idx.reshape(-1)\n",
    "\n",
    "        # Validar por radio mÃ¡ximo\n",
    "        within = dist_rad <= max_distance_rad\n",
    "\n",
    "        # Seleccionar filas del CSV por Ã­ndice de vecino\n",
    "        matched = df_csv.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "        # Convertir distancia a metros\n",
    "        dist_m = dist_rad * R_EARTH_M\n",
    "\n",
    "        # Preparar columnas a anexar (todas las del CSV + la distancia)\n",
    "        # Para filas fuera del radio, ponemos NaN\n",
    "        for col in df_csv.columns:\n",
    "            vals = matched[col].copy()\n",
    "            vals[~within] = np.nan\n",
    "            base_chunk[col] = vals.values\n",
    "\n",
    "        base_chunk[f\"dist_m_{alias}\"] = np.where(within, dist_m, np.nan)\n",
    "\n",
    "        print(f\"   â• {alias}: match en {within.sum():,}/{len(within):,} filas (radio â‰¤ {max_distance_km} km)\")\n",
    "\n",
    "    # Guardar chunk\n",
    "    out_path = output_dir / f\"merged_chunk_{start:08d}_{end:08d}.parquet\"\n",
    "    base_chunk.to_parquet(out_path, index=False)\n",
    "    chunk_paths.append(out_path)\n",
    "    print(f\"âœ… Guardado: {out_path.name} ({len(base_chunk):,} filas)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7990b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Combinando chunksâ€¦\n",
      "ğŸ“ Total final: 1,469,652 filas (base tenÃ­a 1,469,652)\n",
      "ğŸ¯ Archivo final: /Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Merged_Delegacion_final_balltree.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============== UNIR CHUNKS Y CHEQUEAR ==============\n",
    "print(\"\\nğŸ“¦ Combinando chunksâ€¦\")\n",
    "final_df = pd.concat((pd.read_parquet(p) for p in chunk_paths), ignore_index=True)\n",
    "print(f\"ğŸ“ Total final: {len(final_df):,} filas (base tenÃ­a {n_rows:,})\")\n",
    "\n",
    "final_df.to_parquet(output_final, index=False)\n",
    "print(f\"ğŸ¯ Archivo final: {output_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f5c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Filas sin ningÃºn match: 457 de 1,469,652 (0.03%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta a tu parquet final\n",
    "output_final = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Merged_Delegacion_final_balltree.parquet\")\n",
    "\n",
    "# Cargar (solo unas columnas para ahorrar memoria)\n",
    "df = pd.read_parquet(output_final)\n",
    "\n",
    "# Detectar todas las columnas nuevas (las que vienen de los CSVs)\n",
    "cols_csv = [c for c in df.columns if any(\n",
    "    alias in c for alias in [\"escuelas_privadas\", \"hospitales\", \"metrobus\", \"stc_metro\", \"areas_verdes\", \"escuelas_publicas\"]\n",
    ")]\n",
    "\n",
    "# Contar cuÃ¡ntas filas tienen TODAS esas columnas vacÃ­as\n",
    "mask_empty = df[cols_csv].isna().all(axis=1)\n",
    "\n",
    "sin_match = mask_empty.sum()\n",
    "total = len(df)\n",
    "print(f\"ğŸ“Š Filas sin ningÃºn match: {sin_match:,} de {total:,} ({sin_match/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c15b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59585e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463471cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b74c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f1184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf239e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc25996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8affb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12366600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456120d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342bfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74abb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da33cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d268e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31361ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Point\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c8d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACIÃ“N ===\n",
    "base_parquet = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Delegacion_comb_filtrado_wgs84.parquet\")\n",
    "carpeta_csvs = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/csv\")\n",
    "csvs_interes = [\n",
    "    \"escuelas_privadas/escuelas_privadas_con_coordenadas.csv\",\n",
    "    \"hospitales_y_centros_de_salud/hospitales_y_centros_de_salud_con_coordenadas.csv\",\n",
    "    \"mb_shp/Metrobus_estaciones_con_coordenadas.csv\",\n",
    "    \"stcmetro_shp/STC_Metro_estaciones_utm14n_con_coordenadas.csv\",\n",
    "    \"areas_verdes_filtrado.csv\",\n",
    "    \"escuelas_publicas.csv\"\n",
    "]\n",
    "\n",
    "output_dir = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/tmp_chunks\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_final = Path(\"/Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Merged_Delegacion_final_1to1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab4009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParÃ¡metros\n",
    "chunk_size = 50_000          # nÃºmero de filas por chunk\n",
    "max_distance_km = 5           # radio mÃ¡ximo de bÃºsqueda (en km)\n",
    "max_distance_m = max_distance_km * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20f9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Archivo base cargado: 1,469,652 filas, 153 columnas\n",
      "âœ… GeometrÃ­a creada: EPSG:4326, 1,469,652 filas\n"
     ]
    }
   ],
   "source": [
    "# === 1ï¸âƒ£ CARGAR ARCHIVO BASE ===\n",
    "base = pd.read_parquet(base_parquet)\n",
    "print(f\"ğŸ“¦ Archivo base cargado: {len(base):,} filas, {len(base.columns)} columnas\")\n",
    "\n",
    "# Crear geometrÃ­a manualmente\n",
    "if not {'lon_wgs84', 'lat_wgs84'}.issubset(base.columns):\n",
    "    raise ValueError(\"âŒ El archivo base debe tener columnas 'lon_wgs84' y 'lat_wgs84'\")\n",
    "\n",
    "base = gpd.GeoDataFrame(\n",
    "    base,\n",
    "    geometry=gpd.points_from_xy(base[\"lon_wgs84\"], base[\"lat_wgs84\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(f\"âœ… GeometrÃ­a creada: {base.crs}, {len(base):,} filas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2dc09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 2ï¸âƒ£ FUNCIONES AUXILIARES ===\n",
    "def preparar_csv(ruta_csv: Path, base_crs):\n",
    "    \"\"\"Carga un CSV con columnas 'latitud' y 'longitud' y lo convierte a GeoDataFrame.\"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "\n",
    "    # Validar columnas esperadas\n",
    "    if not {\"latitud\", \"longitud\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"âš ï¸ El archivo {ruta_csv.name} debe tener columnas 'latitud' y 'longitud'.\")\n",
    "\n",
    "    # Crear geometrÃ­a\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[\"longitud\"], df[\"latitud\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(base_crs)\n",
    "\n",
    "    return gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5124b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Procesando chunk 1/30 (0 â†’ 50,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_1.parquet (66,029 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 2/30 (50,000 â†’ 100,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_2.parquet (67,480 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 3/30 (100,000 â†’ 150,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_3.parquet (64,332 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 4/30 (150,000 â†’ 200,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_4.parquet (70,864 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 5/30 (200,000 â†’ 250,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_5.parquet (72,392 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 6/30 (250,000 â†’ 300,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_6.parquet (70,515 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 7/30 (300,000 â†’ 350,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_7.parquet (71,334 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 8/30 (350,000 â†’ 400,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_8.parquet (69,592 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 9/30 (400,000 â†’ 450,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_9.parquet (63,322 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 10/30 (450,000 â†’ 500,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_10.parquet (65,760 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 11/30 (500,000 â†’ 550,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_11.parquet (66,520 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 12/30 (550,000 â†’ 600,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_12.parquet (70,380 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 13/30 (600,000 â†’ 650,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_13.parquet (71,505 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 14/30 (650,000 â†’ 700,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_14.parquet (72,428 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 15/30 (700,000 â†’ 750,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_15.parquet (69,816 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 16/30 (750,000 â†’ 800,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_16.parquet (66,521 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 17/30 (800,000 â†’ 850,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_17.parquet (62,862 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 18/30 (850,000 â†’ 900,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_18.parquet (65,893 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 19/30 (900,000 â†’ 950,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_19.parquet (58,454 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 20/30 (950,000 â†’ 1,000,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_20.parquet (72,320 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 21/30 (1,000,000 â†’ 1,050,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_21.parquet (69,685 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 22/30 (1,050,000 â†’ 1,100,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_22.parquet (70,603 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 23/30 (1,100,000 â†’ 1,150,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_23.parquet (66,682 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 24/30 (1,150,000 â†’ 1,200,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_24.parquet (68,349 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 25/30 (1,200,000 â†’ 1,250,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_25.parquet (66,942 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 26/30 (1,250,000 â†’ 1,300,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_26.parquet (59,529 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 27/30 (1,300,000 â†’ 1,350,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_27.parquet (63,222 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 28/30 (1,350,000 â†’ 1,400,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_28.parquet (64,536 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 29/30 (1,400,000 â†’ 1,450,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n",
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_29.parquet (66,672 filas)\n",
      "\n",
      "ğŸ§© Procesando chunk 30/30 (1,450,000 â†’ 1,500,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n",
      "ğŸ“ Merge espacial con: hospitales_y_centros_de_salud_con_coordenadas\n",
      "ğŸ“ Merge espacial con: Metrobus_estaciones_con_coordenadas\n",
      "ğŸ“ Merge espacial con: STC_Metro_estaciones_utm14n_con_coordenadas\n",
      "ğŸ“ Merge espacial con: areas_verdes_filtrado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Merge espacial con: escuelas_publicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/v8hk2rj9117bkb38sq4yjyfw0000gn/T/ipykernel_1902/1339388365.py:32: FutureWarning: Passing 'suffixes' which cause duplicate columns {'longitud_left', 'latitud_left'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = gpd.sjoin_nearest(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk guardado: merged_chunk_30.parquet (26,341 filas)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "# === 3ï¸âƒ£ PROCESAR EN CHUNKS (versiÃ³n segura) ===\n",
    "chunks = range(0, len(base), chunk_size)\n",
    "chunk_paths = []\n",
    "\n",
    "for i, offset in enumerate(chunks):\n",
    "    print(f\"\\nğŸ§© Procesando chunk {i+1}/{len(chunks)} ({offset:,} â†’ {offset+chunk_size:,})\")\n",
    "    base_chunk = base.iloc[offset:offset + chunk_size].copy()\n",
    "\n",
    "    # Crear geometrÃ­a si no existe\n",
    "    if 'geometry' not in base_chunk.columns:\n",
    "        base_chunk = gpd.GeoDataFrame(\n",
    "            base_chunk,\n",
    "            geometry=gpd.points_from_xy(base_chunk[\"lon_wgs84\"], base_chunk[\"lat_wgs84\"]),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "    # Convertir a CRS mÃ©trico (para usar distancias en metros)\n",
    "    base_chunk = base_chunk.to_crs(3857)\n",
    "\n",
    "    # === MERGE CON CADA CSV ===\n",
    "    for csv_rel in csvs_interes:\n",
    "        ruta_csv = carpeta_csvs / csv_rel\n",
    "        alias = ruta_csv.stem.replace(\" \", \"_\")\n",
    "\n",
    "        print(f\"ğŸ“ Merge espacial con: {alias}\")\n",
    "        csv_gdf = preparar_csv(ruta_csv, base_chunk.crs)\n",
    "\n",
    "        # Renombrar columnas de coordenadas antes del join\n",
    "        csv_gdf = csv_gdf.rename(columns={\n",
    "            \"latitud\": f\"latitud_{alias}\",\n",
    "            \"longitud\": f\"longitud_{alias}\"\n",
    "        })\n",
    "\n",
    "        # merge left (preserva todas las filas del base_chunk)\n",
    "        merged = gpd.sjoin_nearest(\n",
    "            base_chunk,\n",
    "            csv_gdf,\n",
    "            how=\"left\",\n",
    "            max_distance=max_distance_m,\n",
    "            distance_col=f\"dist_{alias}\",\n",
    "            rsuffix=f\"_{alias}\"\n",
    "        )\n",
    "\n",
    "        # Renombrar columnas nuevas con sufijo\n",
    "        new_cols = [c for c in csv_gdf.columns if c not in ['geometry', 'latitud', 'longitud']]\n",
    "        rename_map = {c: f\"{c}_{alias}\" for c in new_cols if c in merged.columns}\n",
    "        merged.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "        # Eliminar columnas duplicadas de geometrÃ­a\n",
    "        merged = merged.loc[:, ~merged.columns.duplicated(keep='first')]\n",
    "\n",
    "        # ğŸ‘‰ En lugar de reemplazar el chunk completo, actualizamos columnas\n",
    "        base_chunk = merged\n",
    "\n",
    "    # Convertir de nuevo a WGS84 para guardar\n",
    "    base_chunk = base_chunk.to_crs(4326)\n",
    "\n",
    "    # === GUARDAR RESULTADO PARCIAL ===\n",
    "    output_chunk = output_dir / f\"merged_chunk_{i+1}.parquet\"\n",
    "    base_chunk.to_parquet(output_chunk, index=False)\n",
    "    chunk_paths.append(output_chunk)\n",
    "    print(f\"âœ… Chunk guardado: {output_chunk.name} ({len(base_chunk):,} filas)\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f51989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Procesando chunk 1/30 (0 â†’ 50,000)\n",
      "ğŸ“ Merge espacial con: escuelas_privadas_con_coordenadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eaha/Documents/TFM/.venv/lib/python3.12/site-packages/geopandas/array.py:408: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === 3ï¸âƒ£ PROCESAR EN CHUNKS (versiÃ³n sin cambio de CRS, 1:1 real) ===\n",
    "\n",
    "chunks = range(0, len(base), chunk_size)\n",
    "chunk_paths = []\n",
    "\n",
    "# Convertir distancia de km a grados (aprox.)\n",
    "max_distance_deg = max_distance_km / 111\n",
    "\n",
    "for i, offset in enumerate(chunks):\n",
    "    print(f\"\\nğŸ§© Procesando chunk {i+1}/{len(chunks)} ({offset:,} â†’ {offset+chunk_size:,})\")\n",
    "    base_chunk = base.iloc[offset:offset + chunk_size].copy()\n",
    "\n",
    "    # Asegurar que tiene geometrÃ­a vÃ¡lida\n",
    "    if \"geometry\" not in base_chunk.columns:\n",
    "        base_chunk = gpd.GeoDataFrame(\n",
    "            base_chunk,\n",
    "            geometry=gpd.points_from_xy(base_chunk[\"lon_wgs84\"], base_chunk[\"lat_wgs84\"]),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "    # === MERGE CON CADA CAPA CSV ===\n",
    "    for csv_rel in csvs_interes:\n",
    "        ruta_csv = carpeta_csvs / csv_rel\n",
    "        alias = ruta_csv.stem.replace(\" \", \"_\")\n",
    "\n",
    "        print(f\"ğŸ“ Merge espacial con: {alias}\")\n",
    "        csv_gdf = preparar_csv(ruta_csv, base_chunk.crs)\n",
    "\n",
    "        # Eliminar columna conflictiva si quedÃ³ de un merge previo\n",
    "        if \"index_right\" in base_chunk.columns:\n",
    "            base_chunk = base_chunk.drop(columns=[\"index_right\"])\n",
    "\n",
    "        # === JOIN ESPACIAL 1:1 POR PUNTO MÃS CERCANO ===\n",
    "        merged = gpd.sjoin_nearest(\n",
    "            base_chunk,\n",
    "            csv_gdf,\n",
    "            how=\"left\",\n",
    "            max_distance=max_distance_deg,  # usar grados, no metros\n",
    "            distance_col=f\"dist_{alias}\",\n",
    "            rsuffix=f\"_{alias}\"\n",
    "        )\n",
    "\n",
    "        # --- LIMPIEZA Y RENOMBRADO DE COLUMNAS ---\n",
    "        merged = pd.DataFrame(merged)\n",
    "\n",
    "        # Renombrar columnas nuevas con sufijo (excepto coordenadas y geometry)\n",
    "        new_cols = [c for c in csv_gdf.columns if c not in [\"geometry\", \"latitud\", \"longitud\"]]\n",
    "        rename_map = {c: f\"{c}_{alias}\" for c in new_cols if c in merged.columns}\n",
    "        merged.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "        merged = merged.loc[:, ~merged.columns.duplicated()]\n",
    "\n",
    "        # âœ… Conservar todas las filas del base_chunk\n",
    "        base_chunk = pd.merge(\n",
    "            base_chunk,\n",
    "            merged[\n",
    "                [\"lon_wgs84\", \"lat_wgs84\"]\n",
    "                + [col for col in merged.columns if col.startswith(tuple(rename_map.values())) or col.startswith(\"dist_\")]\n",
    "            ],\n",
    "            on=[\"lon_wgs84\", \"lat_wgs84\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Volver a GeoDataFrame\n",
    "        base_chunk = gpd.GeoDataFrame(base_chunk, geometry=gpd.points_from_xy(base_chunk[\"lon_wgs84\"], base_chunk[\"lat_wgs84\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "    # === GUARDAR RESULTADO PARCIAL ===\n",
    "    output_chunk = output_dir / f\"merged_chunk_{i+1}.parquet\"\n",
    "    base_chunk.to_parquet(output_chunk, index=False)\n",
    "    chunk_paths.append(output_chunk)\n",
    "    print(f\"âœ… Chunk guardado: {output_chunk.name} ({len(base_chunk):,} filas)\")\n",
    "\n",
    "# === RESUMEN FINAL ===\n",
    "print(\"\\nğŸ“¦ Procesamiento terminado.\")\n",
    "print(f\"ğŸ§© Se generaron {len(chunk_paths)} archivos parquet en: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaf9e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© Combinando todos los chunks finales...\n",
      "âœ… Total final: 36,573 filas\n",
      "ğŸ¯ Archivo final guardado en: /Users/eaha/Documents/TFM/mlops-repo/data/processed/AGEBvsCatastral/Merged_Delegacion_final_1to1.parquet\n"
     ]
    }
   ],
   "source": [
    "# === 4ï¸âƒ£ UNIR TODOS LOS CHUNKS ===\n",
    "print(\"\\nğŸ§© Combinando todos los chunks finales...\")\n",
    "df_final = pd.concat([pd.read_parquet(p) for p in chunk_paths], ignore_index=True)\n",
    "print(f\"âœ… Total final: {len(df_final):,} filas\")\n",
    "\n",
    "df_final.to_parquet(output_final, index=False)\n",
    "print(f\"ğŸ¯ Archivo final guardado en: {output_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
